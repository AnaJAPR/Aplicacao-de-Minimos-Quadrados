{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho A2 de Álgebra Linear \n",
    "\n",
    "\n",
    "### Ana Júlia Amaro Pereira Rocha\n",
    "### Maria Eduarda Mesquita Magalhães\n",
    "\n",
    "### **Curso: Ciência de Dados e Inteligência Artificial / 2º período**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tema**: Aplicação de mínimos quadrados com intuito de fazer uma regressão linear nos dados de um Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Em Álgebra Linear, aprendemos que nem sempre um sistema linear possui soluções, especialmente quando tem-se mais equações do que incógnitas. Desse modo, uma forma de \"resolver\" tal problema é usando o método dos mínimos quadrados, no qual encontramos uma reta que melhor minimiza o erro no conjunto de dados. Logo, o objetivo desse trabalho é justamente analisar duas colunas da base de dados, em cada gráfico plotado, buscando entender a relação entre tais colunas. Com isso, chegaremos a conclusões sobre o tema do Data Frame a partir do uso de álgebra linear em seus dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from csv and dropping null values\n",
    "df = pd.read_csv(\"Sleep_Efficiency.csv\").dropna()\n",
    "\n",
    "# Creating x and y vectors with values from the Deep sleep percentage and Light sleep percentage columns\n",
    "# the reshape fit the values in a two-dimensional array for future multiplications\n",
    "x = df[\"Deep sleep percentage\"].values.reshape(-1,1) \n",
    "y = df[\"Light sleep percentage\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para os próximos blocos de código, deve-se levar em conta algo que aprendemos em Álgebra Linear, ou seja, que quando Ax=b for inconsistente, sua solução com mínimos quadrados minimizará ||Ax - b||² e isso é feito por meio das chamadas \"equações normais\" $ A^T A \\hat{x} = A^T b $.\n",
    "#### Tendo isso em mente, vamos explicar aos poucos o que estamos fazendo em cada bloco de código. No caso da linha seguinte, estamos adicionando uma coluna de uns na matriz bidimensional com valores x (valores da coluna \"Deep sleep percentage\"). Essa adição é feita para representar o termo linear, considerando que temos $ Xc = y $ onde **c** é um vetor coluna com os coeficientes **b** e **a**, respectivamente, de $ ax + b = y $ e $y$ é o vetor definido anteriormente com dados da coluna \"Light sleep percentage\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column of 1s to represent the linear term\n",
    "X = np.column_stack([np.ones_like(x), x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora, observe que no nosso caso $ A^T A \\hat{x} = A^T b $ torna-se $ X^T X \\hat{c} = X^T y $ sendo $ \\hat{c} $ a solução procurada, ou seja, o vetor com os coeficientes de uma equação linear que melhor representa os dados em questão. \n",
    "#### Outro ponto notório que aprendemos em Álgebra Linear é que $ X^T X $ será inversível exatamente quando as colunas de X forem linearmente independentes (LI) e, dessa maneira, o melhor c estimado seria $ c = (X^T X)^{-1} X^T y$.\n",
    "#### Logo, como a matriz X possui apenas duas colunas, sendo uma com todas as entradas iguais a 1 e a outra com valores \"aleatórios\" de \"Deep sleep percentage\" da base de dados usada, temos que essas colunas são LI, já que uma coluna tem todos os valores iguais e a outra não, então não há um valor que multiplicando a coluna 1 encontremos a coluna 2.\n",
    "#### Portanto, podemos usar a expressão $ \\hat{c} = (X^T X)^{-1} X^T y $ para encontrar o vetor solução $ \\hat{c} $  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the linear regression using matrix solution\n",
    "coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Angular and linear coefficients\n",
    "l_coeff, a_coeff = coefficients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
